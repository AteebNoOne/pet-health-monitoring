{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1f10FLevjGkFpHCYSRwCSKp2ZxTMOmSlD","authorship_tag":"ABX9TyMOVBqBLcSszSFf082P9qbU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0IVCiF1qqppJ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","from torch.optim import Adam"]},{"cell_type":"code","source":["train_dir = \"/content/drive/MyDrive/CAT/train\"\n","val_dir   = \"/content/drive/MyDrive/CAT/val\"\n","test_dir  = \"/content/drive/MyDrive/CAT/test\""],"metadata":{"id":"yjK8a5Bwq9GV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tf = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","])\n","\n","val_tf = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","])"],"metadata":{"id":"gGBARBbVrmKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n","val_ds   = datasets.ImageFolder(val_dir, transform=val_tf)\n","test_ds  = datasets.ImageFolder(test_dir, transform=val_tf)\n","\n","train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n","val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)\n","test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n"],"metadata":{"id":"oiVrMLiirxqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = models.resnet18(pretrained=True)\n","model.fc = nn.Linear(model.fc.in_features, 3)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmfCOStcr8Ga","executionInfo":{"status":"ok","timestamp":1764270022480,"user_tz":420,"elapsed":868,"user":{"displayName":"Nida Imran","userId":"11290324996029625410"}},"outputId":"d7a314ee-e963-452a-f6b8-1cfa409c498d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 44.7M/44.7M [00:00<00:00, 157MB/s]\n"]}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=0.0001)\n"],"metadata":{"id":"8080r9vFsC3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 12\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for imgs, labels in train_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        out = model(imgs)\n","        loss = criterion(out, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        pred = torch.argmax(out, dim=1)\n","        correct += (pred == labels).sum().item()\n","        total += labels.size(0)\n","\n","    acc = correct / total\n","    print(f\"Epoch {epoch+1}/{EPOCHS}  Loss={running_loss/len(train_loader):.3f}  Acc={acc:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-wsVtxkGsMTH","executionInfo":{"status":"ok","timestamp":1764276199123,"user_tz":420,"elapsed":6100289,"user":{"displayName":"Nida Imran","userId":"11290324996029625410"}},"outputId":"7495e90f-5d77-46fe-c23e-28ccbfaf3b00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/12  Loss=0.759  Acc=0.660\n","Epoch 2/12  Loss=0.375  Acc=0.855\n","Epoch 3/12  Loss=0.212  Acc=0.925\n","Epoch 4/12  Loss=0.156  Acc=0.944\n","Epoch 5/12  Loss=0.133  Acc=0.963\n","Epoch 6/12  Loss=0.111  Acc=0.958\n","Epoch 7/12  Loss=0.104  Acc=0.961\n","Epoch 8/12  Loss=0.085  Acc=0.964\n","Epoch 9/12  Loss=0.106  Acc=0.963\n","Epoch 10/12  Loss=0.098  Acc=0.969\n","Epoch 11/12  Loss=0.052  Acc=0.980\n","Epoch 12/12  Loss=0.051  Acc=0.985\n"]}]},{"cell_type":"code","source":["model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for imgs, labels in test_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","\n","        outputs = model(imgs)\n","        pred = torch.argmax(outputs, dim=1)\n","        correct += (pred == labels).sum().item()\n","        total += labels.size(0)\n","\n","test_acc = correct / total\n","print(\"TEST Accuracy =\", test_acc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdrBDJnLJUuH","executionInfo":{"status":"ok","timestamp":1764277791679,"user_tz":420,"elapsed":80872,"user":{"displayName":"Nida Imran","userId":"11290324996029625410"}},"outputId":"e29ac2b9-3701-43d0-983a-495470a1ae96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TEST Accuracy = 0.8547008547008547\n"]}]},{"cell_type":"code","source":["\n","\n","model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for imgs, labels in val_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        outputs = model(imgs)\n","        preds = torch.argmax(outputs, dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","val_acc = correct / total\n","print(f\"Validation Accuracy = {val_acc:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-iN29YNYK4_U","executionInfo":{"status":"ok","timestamp":1764278278268,"user_tz":420,"elapsed":158670,"user":{"displayName":"Nida Imran","userId":"11290324996029625410"}},"outputId":"6351c0bf-b9b2-4bf0-8822-a14a35389d60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy = 0.8688\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/CAT/model\")\n","print(\"Model training complete & saved!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iXBSpgGwMXTY","executionInfo":{"status":"ok","timestamp":1764278556323,"user_tz":420,"elapsed":214,"user":{"displayName":"Nida Imran","userId":"11290324996029625410"}},"outputId":"300e1cd6-cf14-48e9-d69d-78fa4a2b54a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model training complete & saved!\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","import torch\n","from torchvision import transforms, models\n","import torch.nn as nn\n","\n","classes = [\"angry\", \"happy\", \"sad\"]\n","\n","model = models.resnet18(pretrained=False)\n","model.fc = nn.Linear(model.fc.in_features, 3)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/CAT/model\", map_location=\"cpu\"))\n","model.eval()\n","\n","tf = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","])\n","\n","def predict(img_path):\n","    img = Image.open(img_path).convert(\"RGB\")\n","    x = tf(img).unsqueeze(0)\n","\n","    with torch.no_grad():\n","        out = model(x)\n","    label = torch.argmax(out, dim=1).item()\n","    return classes[label]\n","\n","print(predict(\"/content/drive/MyDrive/CAT/test/happy/1047_jpg.rf.b72d660338fc0ea07b6f5136b64e8a8b.jpg\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSqA44XRMxbb","executionInfo":{"status":"ok","timestamp":1764354534728,"user_tz":420,"elapsed":18424,"user":{"displayName":"Nida Imran","userId":"11290324996029625410"}},"outputId":"fb5960ee-ad0d-4210-d722-55be50c067b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["happy\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","!pip install efficientnet_pytorch\n","from efficientnet_pytorch import EfficientNet\n","\n","train_dir = \"/content/drive/MyDrive/CAT/train\"\n","val_dir   = \"/content/drive/MyDrive/CAT/val\"\n","test_dir  = \"/content/drive/MyDrive/CAT/test\"\n","\n","train_tf = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n","    transforms.ToTensor(),\n","])\n","\n","val_tf = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","])\n","\n","\n","train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n","val_ds   = datasets.ImageFolder(val_dir, transform=val_tf)\n","test_ds  = datasets.ImageFolder(test_dir, transform=val_tf)\n","\n","train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n","val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)\n","test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n","\n","model = EfficientNet.from_pretrained('efficientnet-b0')\n","model._fc = nn.Linear(model._fc.in_features, 3)  # 3 classes: happy/angry/sad\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=1e-4)\n","\n","\n","EPOCHS = 15\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for imgs, labels in train_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(imgs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        preds = torch.argmax(outputs, dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    train_acc = correct / total\n","    print(f\"Epoch {epoch+1}/{EPOCHS}  Loss={running_loss/len(train_loader):.3f}  Train Acc={train_acc:.3f}\")\n","\n","\n","    # Validation Accuracy\n","\n","    model.eval()\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for imgs, labels in val_loader:\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            outputs = model(imgs)\n","            preds = torch.argmax(outputs, dim=1)\n","            val_correct += (preds == labels).sum().item()\n","            val_total += labels.size(0)\n","    val_acc = val_correct / val_total\n","    print(f\"Validation Acc={val_acc:.3f}\\n\")\n","    model.train()\n","\n","\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/CAT/cat_emotion_efficientnet.pth\")\n","print(\"EfficientNet-B0 model saved as cat_emotion_efficientnet.pth\")\n","\n","\n","# TEST ACCURACY\n","\n","model.eval()\n","test_correct = 0\n","test_total = 0\n","with torch.no_grad():\n","    for imgs, labels in test_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        outputs = model(imgs)\n","        preds = torch.argmax(outputs, dim=1)\n","        test_correct += (preds == labels).sum().item()\n","        test_total += labels.size(0)\n","test_acc = test_correct / test_total\n","print(f\"TEST Accuracy = {test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D839P1eOPzoV","outputId":"0e4906dd-4387-484b-bce3-78f0e082e990","executionInfo":{"status":"ok","timestamp":1764356279135,"user_tz":420,"elapsed":506122,"user":{"displayName":"Nida Imran","userId":"11290324996029625410"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.12/dist-packages (0.7.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from efficientnet_pytorch) (2.9.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->efficientnet_pytorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->efficientnet_pytorch) (3.0.3)\n","Loaded pretrained weights for efficientnet-b0\n","Epoch 1/15  Loss=1.000  Train Acc=0.511\n","Validation Acc=0.613\n","\n","Epoch 2/15  Loss=0.685  Train Acc=0.756\n","Validation Acc=0.753\n","\n","Epoch 3/15  Loss=0.452  Train Acc=0.832\n","Validation Acc=0.849\n","\n","Epoch 4/15  Loss=0.315  Train Acc=0.891\n","Validation Acc=0.877\n","\n","Epoch 5/15  Loss=0.218  Train Acc=0.926\n","Validation Acc=0.901\n","\n","Epoch 6/15  Loss=0.151  Train Acc=0.948\n","Validation Acc=0.886\n","\n","Epoch 7/15  Loss=0.136  Train Acc=0.957\n","Validation Acc=0.886\n","\n","Epoch 8/15  Loss=0.133  Train Acc=0.953\n","Validation Acc=0.905\n","\n","Epoch 9/15  Loss=0.102  Train Acc=0.971\n","Validation Acc=0.908\n","\n","Epoch 10/15  Loss=0.085  Train Acc=0.974\n","Validation Acc=0.901\n","\n","Epoch 11/15  Loss=0.084  Train Acc=0.972\n","Validation Acc=0.905\n","\n","Epoch 12/15  Loss=0.085  Train Acc=0.966\n","Validation Acc=0.910\n","\n","Epoch 13/15  Loss=0.083  Train Acc=0.972\n","Validation Acc=0.892\n","\n","Epoch 14/15  Loss=0.069  Train Acc=0.979\n","Validation Acc=0.899\n","\n","Epoch 15/15  Loss=0.074  Train Acc=0.977\n","Validation Acc=0.914\n","\n","EfficientNet-B0 model saved as cat_emotion_efficientnet.pth\n","TEST Accuracy = 0.9359\n"]}]}]}